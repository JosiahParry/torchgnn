[{"path":"https://josiahparry.github.io/torchgnn/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Josiah Parry. Author, maintainer.","code":""},{"path":"https://josiahparry.github.io/torchgnn/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Parry J (2025). torchgnn: Graph Neural Network Extensions 'torch'. R package version 0.0.0.9000, https://josiahparry.github.io/torchgnn/.","code":"@Manual{,   title = {torchgnn: Graph Neural Network Extensions for `torch`},   author = {Josiah Parry},   year = {2025},   note = {R package version 0.0.0.9000},   url = {https://josiahparry.github.io/torchgnn/}, }"},{"path":"https://josiahparry.github.io/torchgnn/index.html","id":"torchgnn","dir":"","previous_headings":"","what":"Graph Neural Network Extensions for `torch`","title":"Graph Neural Network Extensions for `torch`","text":"Graph Neural Networks {torch} R.","code":""},{"path":"https://josiahparry.github.io/torchgnn/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Graph Neural Network Extensions for `torch`","text":"Install development version:","code":"pak::pak(\"josiahparry/torchgnn\")"},{"path":"https://josiahparry.github.io/torchgnn/index.html","id":"features","dir":"","previous_headings":"","what":"Features","title":"Graph Neural Network Extensions for `torch`","text":"following layers implemented gcn_conv_layer(): Standard GCN layer (Kipf & Welling, 2016) gcn_general_layer(): Generalized GCN layer (Hamilton, 2020) sage_layer(): GraphSAGE layer (Hamilton, Ying, Leskovec, 2017) regconv_layer(): RegionConv layer regionalized GCN (Guo et al. 2025)","code":""},{"path":"https://josiahparry.github.io/torchgnn/index.html","id":"models","dir":"","previous_headings":"Features","what":"Models","title":"Graph Neural Network Extensions for `torch`","text":"torchgnn provides gcn_conv_model() gcn_general_model() utilities create GCN models multiple layers.","code":"library(torchgnn)  gcn_general_model(   in_features = 50,   hidden_dims = c(32, 16),   out_features = 1 ) An `nn_module` containing 4,305 parameters.  ── Modules ───────────────────────────────────────────────────────────────────── • layers: <nn_module_list> #4,305 parameters"},{"path":"https://josiahparry.github.io/torchgnn/index.html","id":"aggregators","dir":"","previous_headings":"Features","what":"Aggregators","title":"Graph Neural Network Extensions for `torch`","text":"S7 class torchgnn::Aggregator created generic method signature forward(x, adj, tensor). basic aggregators implemented present. SumAggregator(): Sum neighbor features MeanAggregator(): Mean neighbor features (row normalization) MaxAggregator(): Element-wise maximum neighbor features MinAggregator(): Element-wise minimum neighbor features ProductAggregator(): Element-wise product neighbor features VarAggregator(): Variance neighbor features StdAggregator(): Standard deviation neighbor features","code":"MeanAggregator() <torchgnn::MeanAggregator>  @ name     : chr \"mean\"  @ learnable: logi FALSE"},{"path":"https://josiahparry.github.io/torchgnn/index.html","id":"data-preparation","dir":"","previous_headings":"Features","what":"Data Preparation","title":"Graph Neural Network Extensions for `torch`","text":"torchgnn works sparse COO torch tensors adjacency expects dense feature tensors. Note weighted handled incorporating directly adjacency matrix. adj_from_edgelist(, , weight): Create sparse adjacency matrix edge list (supports character IDs, ) nodes_to_tensor(nodes, adj, node_id) convert node features dataframe tensor correct ordering. function graph_split() create split masks. API likely change integrate large graphs torch luz figured .","code":"set.seed(0) i <- sort(rep.int(1:10, 5)) j <- sample(1:10, 50, TRUE)  adj <- adj_from_edgelist(i, j) adj torch_tensor [ SparseCPUFloatType{} indices: Columns 1 to 26 0  0  0  0  0  0  1  1  1  1  1  2  2  2  2  2  3  3  3  3  3  4  4  4  4  4  0  1  3  5  6  8  0  1  2  4  6  1  4  5  6  9  0  4  5  6  8  1  2  3  4  8  Columns 27 to 52 4  5  5  5  5  5  5  5  5  6  6  6  6  6  6  6  6  7  7  7  7  7  8  8  8  8  9  0  2  3  5  6  7  8  9  0  1  2  3  5  7  8  9  5  6  7  8  9  0  3  4  5  Columns 53 to 63 8  8  8  8  9  9  9  9  9  9  9  6  7  8  9  2  4  5  6  7  8  9 [ CPULongType{2,63} ] values:  2  2  1  1  1  1  2  2  1  2  1  1  1  2  1  3  1  2 ... [the output was truncated (use n=-1 to disable)] ] train_data <- as.data.frame(   matrix(runif(10 * 5, max = 20), nrow = 10) )  nodes_to_tensor(train_data, adj) torch_tensor  13.0174   9.5270  15.1417  15.5783  12.6699   5.1603  17.8440   4.0538  15.9462   4.2642   9.5709  17.2868  14.2224   9.1055   2.5874  15.3262   7.7998   2.4338   8.2017   9.5624   1.6849  15.5464   4.9098  16.2174  18.4815  17.5064  19.2124   2.8661  12.0987  11.9752   6.7815   8.6932   4.7926  13.0945  19.5234  16.7888  14.2503   1.1787   7.0639  14.6358   6.9337   7.9999  12.8458   5.4052   7.1345   6.6755   6.5070  17.5254  19.8537   8.6295 [ CPUFloatType{10,5} ] graph_split(adj) <graph_split> named list [1:4]  $ data    :Float [1:10, 1:10] $ train_id: int [1:6] 2 7 4 3 5 10 $ val_id  : int [1:2] 6 1 $ test_id : int [1:2] 9 8 @ prop: num [1:3] 0.6 0.2 0.2"},{"path":"https://josiahparry.github.io/torchgnn/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Graph Neural Network Extensions for `torch`","text":"Train GCN PubMed Diabetes citation network document classification. example creates GCN model","code":"library(torch) library(torchgnn) library(nanoparquet) library(dplyr)  # read to tempfiles # data from https://linqs.org/datasets/#pubmed-diabetes nodes_tmp <- tempfile(\"nodes\", fileext = \".parquet\") edges_tmp <- tempfile(\"edges\", fileext = \".parquet\")  download.file(   \"https://github.com/JosiahParry/torchgnn/raw/refs/heads/main/examples/data/pubmed-diabetes/edges.parquet\",   edges_tmp, quiet = TRUE ) download.file(   \"https://github.com/JosiahParry/torchgnn/raw/refs/heads/main/examples/data/pubmed-diabetes/nodes.parquet\",   nodes_tmp, quiet = TRUE )  # read in the nodes and edges nodes <- read_parquet(nodes_tmp) edges <- read_parquet(edges_tmp)  # create our adjacency matrix from IDs adj <- adj_from_edgelist(edges$from, edges$to)  # create our training data tensor X <- nodes_to_tensor(   select(nodes, -label),   adj,   node_id = \"paper_id\" )  # create our target class tensor Y <- nodes_to_tensor(select(nodes, 1:2), adj, \"paper_id\")$to(   dtype = torch_long() )$squeeze()  # perform a train, test, validation split split <- graph_split(X, seed = 42) # define a 2 layer GCN model model <- gcn_conv_model(   # number of variables   in_features = 500,   # define our hidden layers   hidden_dims = 16,   # predicting 3 labels   out_features = 3,   dropout = 0.5 ) model An `nn_module` containing 8,067 parameters.  ── Modules ───────────────────────────────────────────────────────────────────── • layers: <nn_module_list> #8,067 parameters # use ADAM optimizer optimizer <- optim_adam(model$parameters, lr = 0.01)  # set our number of epochs n_epochs <- 100  # create our training loop for (epoch in 1:n_epochs) {   model$train()   optimizer$zero_grad()    out <- model(X, adj)   loss <- nnf_cross_entropy(out[split$train_id, ], Y[split$train_id])    loss$backward()   optimizer$step()    model$eval()   with_no_grad({     val_out <- model(X, adj)     val_pred <- val_out[split$val_id, ]$argmax(dim = 2)     val_acc <- (val_pred == Y[split$val_id])$to(       dtype = torch_float()     )$mean()$item()   })    # print info every 20 epochs   if (epoch %% 20 == 0) {     cat(sprintf(       \"Epoch %d | Loss: %.4f | Val Acc: %.4f\\n\",       epoch,       loss$item(),       val_acc     ))   } } Epoch 20 | Loss: 0.8120 | Val Acc: 0.7984 Epoch 40 | Loss: 0.5614 | Val Acc: 0.8377 Epoch 60 | Loss: 0.4601 | Val Acc: 0.8570 Epoch 80 | Loss: 0.4188 | Val Acc: 0.8630 Epoch 100 | Loss: 0.3978 | Val Acc: 0.8656 model$eval() with_no_grad({   test_out <- model(X, adj)   test_pred <- test_out[split$test_id, ]$argmax(dim = 2)   test_acc <- (test_pred == Y[split$test_id])$to(     dtype = torch_float()   )$mean()$item() })  cat(sprintf(\"Test Accuracy: %.4f\\n\", test_acc)) Test Accuracy: 0.8608"},{"path":"https://josiahparry.github.io/torchgnn/reference/adj_from_edgelist.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Sparse Adjacency Matrix from Edge List — adj_from_edgelist","title":"Create Sparse Adjacency Matrix from Edge List — adj_from_edgelist","text":"Create Sparse Adjacency Matrix Edge List","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/adj_from_edgelist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Sparse Adjacency Matrix from Edge List — adj_from_edgelist","text":"","code":"adj_from_edgelist(from, to, weight = NULL, n = NULL, symmetric = TRUE)"},{"path":"https://josiahparry.github.io/torchgnn/reference/adj_from_edgelist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Sparse Adjacency Matrix from Edge List — adj_from_edgelist","text":"Integer character vector source nodes Integer character vector target nodes weight Numeric vector edge weights n Number nodes symmetric Make adjacency symmetric","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/adj_from_edgelist.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Sparse Adjacency Matrix from Edge List — adj_from_edgelist","text":"Sparse COO tensor attributes node_ids id_map","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/adjacency.html","id":null,"dir":"Reference","previous_headings":"","what":"Add self-loops to a graph — gcn_normalize","title":"Add self-loops to a graph — gcn_normalize","text":"Add self-loops graph","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/adjacency.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add self-loops to a graph — gcn_normalize","text":"","code":"gcn_normalize(adj)  adj_row_normalize(adj)  add_graph_self_loops(adj)"},{"path":"https://josiahparry.github.io/torchgnn/reference/adjacency.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add self-loops to a graph — gcn_normalize","text":"adj sparse COO tensor adjacency matrix. Can weighted.","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/aggregator.html","id":null,"dir":"Reference","previous_headings":"","what":"Message Passing Aggregators — Aggregator","title":"Message Passing Aggregators — Aggregator","text":"Aggregators combine neighbor node features graph neural networks. aggregator implements different reduction operation (sum, mean, max, etc.) aggregate features neighboring nodes.","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/aggregator.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Message Passing Aggregators — Aggregator","text":"","code":"Aggregator(name = character(0), learnable = logical(0))  SumAggregator()  MeanAggregator()  MaxAggregator()  MinAggregator()  ProductAggregator()  VarAggregator()  StdAggregator()  LSTMAggregator(in_features, hidden_features = NULL)  SoftmaxAggregator(in_features, learn = TRUE)"},{"path":"https://josiahparry.github.io/torchgnn/reference/aggregator.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Message Passing Aggregators — Aggregator","text":"adj Sparse torch tensor n_nodes x n_nodes. Adjacency matrix defining graph structure. Must sparse COO tensor. tensor Torch tensor n_nodes x n_features. Node feature matrix. Can dense sparse. ... Additional arguments passed specific aggregator methods.","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/aggregator.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Message Passing Aggregators — Aggregator","text":"Available aggregators: SumAggregator(): Sum neighbor features MeanAggregator(): Mean neighbor features (row normalization) MaxAggregator(): Element-wise maximum neighbor features MinAggregator(): Element-wise minimum neighbor features ProductAggregator(): Element-wise product neighbor features VarAggregator(): Variance neighbor features StdAggregator(): Standard deviation neighbor features LSTMAggregator(): -imlemented SoftmaxAggregator(): -imlemented","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/gcn_conv_layer.html","id":null,"dir":"Reference","previous_headings":"","what":"GCN Convolutional Layer (Kipf & Welling 2016) — gcn_conv_layer","title":"GCN Convolutional Layer (Kipf & Welling 2016) — gcn_conv_layer","text":"Implements basic GCN layer Kipf & Welling (2016): $$H^{(k)} = \\sigma(\\tilde{} H^{(k-1)} W + b)$$","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/gcn_conv_layer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"GCN Convolutional Layer (Kipf & Welling 2016) — gcn_conv_layer","text":"","code":"gcn_conv_layer(in_features, out_features, bias = TRUE, normalize = TRUE)"},{"path":"https://josiahparry.github.io/torchgnn/reference/gcn_conv_layer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"GCN Convolutional Layer (Kipf & Welling 2016) — gcn_conv_layer","text":"in_features Integer. Number input features per node out_features Integer. Number output features per node bias Logical. Add learnable bias. Default: TRUE normalize Logical. Whether add self-loops compute symmetric normalization --fly. Default: TRUE x Tensor n_nodes x in_features. Node feature matrix adj Tensor n_nodes x n_nodes. Adjacency matrix defining graph structure. Can binary (0/1) weighted. edge_weight provided, adj binary weights applied edge_weight","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/gcn_conv_layer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"GCN Convolutional Layer (Kipf & Welling 2016) — gcn_conv_layer","text":"Tensor n_nodes x out_features. Transformed node features","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/gcn_conv_layer.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"GCN Convolutional Layer (Kipf & Welling 2016) — gcn_conv_layer","text":"\\(\\tilde{} = \\tilde{D}^{-1/2}(+ )\\tilde{D}^{-1/2}\\) symmetrically normalized adjacency matrix self-loops, \\(\\tilde{D}\\) degree matrix \\(+ \\). standard GCN layer uses: Single weight matrix \\(W\\) Symmetric normalization self-loops Optional --fly normalization normalize = TRUE (default), layer computes \\(\\tilde{}\\) --fly input adjacency matrix adding self-loops applying symmetric normalization. normalize = FALSE, must pass pre-normalized adjacency matrix. Parameters: \\(W\\): in_features x out_features learnable weight matrix \\(b\\): out_features learnable bias term (optional)","code":""},{"path":[]},{"path":"https://josiahparry.github.io/torchgnn/reference/gcn_conv_layer.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"GCN Convolutional Layer (Kipf & Welling 2016) — gcn_conv_layer","text":"Kipf, T. N., & Welling, M. (2016). Semi-supervised classification graph convolutional networks. arXiv preprint arXiv:1609.02907. doi:10.48550/arXiv.1609.02907","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/gcn_conv_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Multi-layer GCN Model — gcn_conv_model","title":"Multi-layer GCN Model — gcn_conv_model","text":"Stacks multiple GCN layers create deep graph convolutional network.","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/gcn_conv_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multi-layer GCN Model — gcn_conv_model","text":"","code":"gcn_conv_model(   in_features,   hidden_dims,   out_features,   activation = nnf_relu,   out_activation = NULL,   dropout = 0,   normalize = TRUE )"},{"path":"https://josiahparry.github.io/torchgnn/reference/gcn_conv_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multi-layer GCN Model — gcn_conv_model","text":"in_features Integer. Number input features per node hidden_dims Integer vector. Dimensions hidden layers (length = L) out_features Integer. Number output features (typically 1 regression) activation Function. Activation hidden layers. Default: nnf_relu dropout Numeric. Dropout rate (0-1) applied hidden layer. Default: 0 normalize Logical. Whether add self-loops apply symmetric normalization. Default: TRUE output_activation Function NULL. Activation output layer. Default: NULL x Tensor n_nodes x in_features. Node feature matrix adj Tensor n_nodes x n_nodes. Binary adjacency matrix (0/1) defining graph structure. normalize = TRUE, self-loops added symmetric normalization applied automatically edge_weight Tensor n_nodes x n_nodes NULL. Optional edge weights apply adjacency structure. NULL, treats edges weight 1. Passed layers. Default: NULL","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/gcn_conv_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Multi-layer GCN Model — gcn_conv_model","text":"Tensor n_nodes x out_features. Final predictions","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/gcn_conv_model.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Multi-layer GCN Model — gcn_conv_model","text":"Architecture: L hidden GCN layers configurable activation 1 output GCN layer optional output activation Total layers = length(hidden_dims) + 1 example, hidden_dims = c(56, 56) creates: Layer 1: in_features → 56 (activation) Layer 2: 56 → 56 (activation) Layer 3: 56 → out_features (output_activation) Uses gcn_conv_layer automatically handles adding self-loops symmetric normalization normalize = TRUE.","code":""},{"path":[]},{"path":"https://josiahparry.github.io/torchgnn/reference/gcn_conv_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Multi-layer GCN Model — gcn_conv_model","text":"","code":"if (FALSE) { # \\dontrun{ # Binary classification with sigmoid model <- gcn_model(14, c(56, 56), 1, output_activation = nnf_sigmoid)  # Multi-class with softmax model <- gcn_model(   14,   c(32, 32),   10,   output_activation = function(x) nnf_softmax(x, dim = -1) )  # Regression (no output activation) model <- gcn_model(14, c(64, 64), 1)  # With dropout and tanh activation model <- gcn_model(14, c(56, 56), 1,                    activation = nnf_tanh,                    dropout = 0.5) } # }"},{"path":"https://josiahparry.github.io/torchgnn/reference/gcn_general_layer.html","id":null,"dir":"Reference","previous_headings":"","what":"Generalized GCN Layer (Hamilton 2020) — gcn_general_layer","title":"Generalized GCN Layer (Hamilton 2020) — gcn_general_layer","text":"Implements single Graph Convolutional Network (GCN) layer following Hamilton 2020: $$\\mathbf{H}^{(k)} = \\sigma\\left(\\mathbf{}\\mathbf{H}^{(k-1)}\\mathbf{W}^{(k)}_{\\text{neigh}} + \\mathbf{H}^{(k-1)}\\mathbf{W}^{(k)}_{\\text{self}}\\right)$$ can also written (Guo et al. 2025): $$\\mathbf{X}^{(l)} = \\sigma\\left(\\mathbf{D}^{-1}\\mathbf{}\\mathbf{X}^{(l-1)}\\boldsymbol{\\Theta}^{(l)} + \\mathbf{X}^{(l-1)}\\boldsymbol{\\Phi}^{(l)} + \\boldsymbol{\\Psi}^{(l)}\\right)$$ layer combines: Neighbor aggregation: \\(D^{-1}AX^{(l-1)}\\Theta^{(l)}\\) Self transformation: \\(X^{(l-1)}\\Phi^{(l)}\\) focal node transformation Global bias: \\(\\Psi^{(l)}\\) additive bias term Parameters: \\(\\Theta\\) (theta): in_features x out_features transforms aggregated neighbor features \\(\\Phi\\) (phi): in_features x out_features transforms node's features \\(\\Psi\\) (psi): out_features global bias term (shared across nodes)","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/gcn_general_layer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generalized GCN Layer (Hamilton 2020) — gcn_general_layer","text":"","code":"gcn_general_layer(in_features, out_features, bias = TRUE, normalize = FALSE)"},{"path":"https://josiahparry.github.io/torchgnn/reference/gcn_general_layer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generalized GCN Layer (Hamilton 2020) — gcn_general_layer","text":"in_features Integer. Number input features per node out_features Integer. Number output features per node bias Logical. Add learnable bias term (\\(\\Psi\\)). Default: TRUE x Tensor n_nodes x in_features. Node feature matrix adj Tensor n_nodes x n_nodes. Adjacency matrix. Expected row-normalized \\(D^{-1}\\) \\(D\\) degree matrix. Can binary weighted. layer perform normalization internally","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/gcn_general_layer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generalized GCN Layer (Hamilton 2020) — gcn_general_layer","text":"Tensor n_nodes x out_features. Transformed node features (activation)","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/gcn_general_layer.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generalized GCN Layer (Hamilton 2020) — gcn_general_layer","text":"adjacency matrix expected row-normalized \\(D^{-1}\\) \\(D\\) degree matrix. layer perform normalization internally.","code":""},{"path":[]},{"path":"https://josiahparry.github.io/torchgnn/reference/gcn_general_layer.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Generalized GCN Layer (Hamilton 2020) — gcn_general_layer","text":"Hamilton, W. L. (2020). Graph Representation Learning. Synthesis Lectures Artificial Intelligence Machine Learning. Springer International Publishing. doi:10.1007/978-3-031-01588-5 Guo, H., Wang, H., Zhu, D., Wu, L., Fotheringham, . S., & Liu, Y. (2025). RegionGCN: Spatial-Heterogeneity-Aware Graph Convolutional Networks. Annals American Association Geographers, 1–17. doi:10.1080/24694452.2025.2558661","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/gcn_general_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Multi-layer Generalized GCN Model (Hamilton 2020) — gcn_general_model","title":"Multi-layer Generalized GCN Model (Hamilton 2020) — gcn_general_model","text":"Stacks multiple generalized GCN layers neighbor/self weight separation. Optional row-normalization applied internally normalize = TRUE.","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/gcn_general_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multi-layer Generalized GCN Model (Hamilton 2020) — gcn_general_model","text":"","code":"gcn_general_model(   in_features,   hidden_dims,   out_features,   activation = nnf_relu,   out_activation = NULL,   dropout = 0,   normalize = TRUE )"},{"path":"https://josiahparry.github.io/torchgnn/reference/graph_split.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Train/Validation/Test Split for Graph Data — graph_split","title":"Create Train/Validation/Test Split for Graph Data — graph_split","text":"Creates splits graph neural networks following rsample's structure. Unlike traditional ML, GCNs use full graph training compute loss labeled (training) nodes.","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/graph_split.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Train/Validation/Test Split for Graph Data — graph_split","text":"","code":"graph_split(data, prop = c(0.6, 0.2, 0.2), seed = NULL)"},{"path":"https://josiahparry.github.io/torchgnn/reference/graph_split.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Train/Validation/Test Split for Graph Data — graph_split","text":"data Data frame adjacency matrix. full dataset split. prop Numeric vector length 2 3. Proportions splits. Length 2: c(train, test) - creates train/test split Length 3: c(train, val, test) - creates train/val/test split Must sum 1.0. Default: c(0.6, 0.2, 0.2) seed Integer NULL. Random seed reproducibility. Default: NULL","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/graph_split.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Train/Validation/Test Split for Graph Data — graph_split","text":"graph_split object (list) containing: data: Original data train_id: Integer vector training indices val_id: Integer vector validation indices (NULL length(prop) == 2) test_id: Integer vector test indices","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/graph_split.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create Train/Validation/Test Split for Graph Data — graph_split","text":"proportions must sum 1.0. function creates non-overlapping splits row belongs exactly one split. GCN training, use: Full X A_sparse forward passes IDs select predictions use loss computation","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/graph_split.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create Train/Validation/Test Split for Graph Data — graph_split","text":"","code":"if (FALSE) { # \\dontrun{ # Standard 60/20/20 split split <- graph_split(A_sparse, seed = 42)  # Custom split (70/15/15) split <- graph_split(A_sparse, prop = c(0.7, 0.15, 0.15))  # Two-way split (80/20 train/test) split <- graph_split(A_sparse, prop = c(0.8, 0.2))  # Use in training predictions <- model(X, A_sparse) train_loss <- nnf_mse_loss(predictions[split$train_id], y[split$train_id]) } # }"},{"path":"https://josiahparry.github.io/torchgnn/reference/nodes_to_tensor.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert Node Features to Tensor — nodes_to_tensor","title":"Convert Node Features to Tensor — nodes_to_tensor","text":"Convert Node Features Tensor","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/nodes_to_tensor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert Node Features to Tensor — nodes_to_tensor","text":"","code":"nodes_to_tensor(nodes, adj = NULL, node_id = NULL)"},{"path":"https://josiahparry.github.io/torchgnn/reference/nodes_to_tensor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert Node Features to Tensor — nodes_to_tensor","text":"nodes Dataframe node features adj Adjacency matrix id_map attribute node_id Column name node IDs","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/nodes_to_tensor.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert Node Features to Tensor — nodes_to_tensor","text":"Dense tensor shape [n_nodes, n_features]","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/regconv_layer.html","id":null,"dir":"Reference","previous_headings":"","what":"Regional GCN Convolutional Layer (Guo et al. 2025) — regconv_layer","title":"Regional GCN Convolutional Layer (Guo et al. 2025) — regconv_layer","text":"Implements Regional Graph Convolutional Network (RegConv) layer Guo et al. (2025): $$\\mathbf{X}^{(l)} = \\sigma\\left(\\left(\\mathbf{D}^{-1}\\mathbf{}\\mathbf{X}^{(l-1)}\\boldsymbol{\\Theta}^{(l)} + \\mathbf{X}^{(l-1)}\\boldsymbol{\\Phi}^{(l)}\\right)\\boldsymbol{\\Omega}_{reg}^{(l)} + \\boldsymbol{\\Psi}_{reg}^{(l)}\\right)$$ layer extends standard GCN layer introducing region-specific parameters handle spatial heterogeneity (spatial regimes). computation two stages: Base GCN transformation: \\(\\mathbf{D}^{-1}\\mathbf{}\\mathbf{X}^{(l-1)}\\boldsymbol{\\Theta}^{(l)} + \\mathbf{X}^{(l-1)}\\boldsymbol{\\Phi}^{(l)}\\) Region-specific modulation: Element-wise multiplication \\(\\boldsymbol{\\Omega}_{reg}\\) addition \\(\\boldsymbol{\\Psi}_{reg}\\) Parameters: \\(\\Theta\\) (theta): in_features x out_features transforms aggregated neighbor features (global) \\(\\Phi\\) (phi): in_features x out_features transforms node's features (global) \\(\\Omega_{reg}\\) (omega_reg): n_regions x out_features region-specific weight modulation \\(\\Psi_{reg}\\) (psi_reg): n_regions x out_features region-specific bias terms","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/regconv_layer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Regional GCN Convolutional Layer (Guo et al. 2025) — regconv_layer","text":"","code":"regconv_layer(in_features, out_features, n_regions)"},{"path":"https://josiahparry.github.io/torchgnn/reference/regconv_layer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Regional GCN Convolutional Layer (Guo et al. 2025) — regconv_layer","text":"in_features Integer. Number input features per node out_features Integer. Number output features per node n_regions Integer. Number spatial regions/regimes x Tensor n_nodes x in_features. Node feature matrix adj Tensor n_nodes x n_nodes. Adjacency matrix. Expected row-normalized \\(D^{-1}\\) \\(D\\) degree matrix. Can binary weighted region_assignments Tensor n_nodes. Integer vector values 1:n_regions, indicating region node belongs . Multiple nodes can belong region edge_weight Tensor n_nodes x n_nodes NULL. Optional edge weights apply adjacency matrix. NULL, uses values adj. Default: NULL","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/regconv_layer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Regional GCN Convolutional Layer (Guo et al. 2025) — regconv_layer","text":"Tensor n_nodes x out_features. Transformed node features (activation)","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/regconv_layer.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Regional GCN Convolutional Layer (Guo et al. 2025) — regconv_layer","text":"RegConv layer designed two-stage training: Stage 1: Train global GCN learn \\(\\Theta\\) \\(\\Phi\\), freeze parameters. Stage 2: Initialize \\(\\Omega_{reg}\\) 1s train region-specific parameters (\\(\\Omega_{reg}\\), \\(\\Psi_{reg}\\)) keeping \\(\\Theta\\) \\(\\Phi\\) fixed. region-specific parameters allow model adjust predictions differently across spatial regimes, enabling model capture spatial heterogeneity.","code":""},{"path":[]},{"path":"https://josiahparry.github.io/torchgnn/reference/regconv_layer.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Regional GCN Convolutional Layer (Guo et al. 2025) — regconv_layer","text":"Guo, H., Wang, H., Zhu, D., Wu, L., Fotheringham, . S., & Liu, Y. (2025). RegionGCN: Spatial-Heterogeneity-Aware Graph Convolutional Networks. Annals American Association Geographers, 1–17. doi:10.1080/24694452.2025.2558661","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/sage_layer.html","id":null,"dir":"Reference","previous_headings":"","what":"GraphSAGE Layer (Hamilton et al. 2017) — sage_layer","title":"GraphSAGE Layer (Hamilton et al. 2017) — sage_layer","text":"Implements GraphSAGE (Graph Sample Aggregate) layer: $$\\mathbf{h}_{\\mathcal{N}(v)}^{(k)} = \\text{AGGREGATE}\\left(\\{\\mathbf{h}_u^{(k-1)} : u \\\\mathcal{N}(v)\\}\\right)$$ $$\\mathbf{h}_v^{(k)} = \\sigma\\left(\\mathbf{W}^{(k)} \\cdot \\text{CONCAT}\\left(\\mathbf{h}_v^{(k-1)}, \\mathbf{h}_{\\mathcal{N}(v)}^{(k)}\\right)\\right)$$ layer: Aggregates neighbor features using specified aggregator Concatenates node's features aggregated neighbor features Applies linear transformation optional normalization Parameters: \\(W\\): (in_features + aggregated_features) x out_features learnable weight matrix \\(b\\): out_features learnable bias term (optional)","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/sage_layer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"GraphSAGE Layer (Hamilton et al. 2017) — sage_layer","text":"","code":"sage_layer(   in_features,   out_features,   aggregator = MeanAggregator(),   bias = TRUE,   concat = TRUE )"},{"path":"https://josiahparry.github.io/torchgnn/reference/sage_layer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"GraphSAGE Layer (Hamilton et al. 2017) — sage_layer","text":"in_features Integer. Number input features per node out_features Integer. Number output features per node aggregator Aggregator S7 object. Default: MeanAggregator() bias Logical. Add learnable bias. Default: TRUE concat Logical. TRUE, concatenates self neighbor features. FALSE, adds . Default: TRUE x Tensor n_nodes x in_features. Node feature matrix (dense sparse) adj Sparse torch tensor n_nodes x n_nodes. Adjacency matrix defining graph structure. Must sparse COO tensor.","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/sage_layer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"GraphSAGE Layer (Hamilton et al. 2017) — sage_layer","text":"Tensor n_nodes x out_features. Transformed node features","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/sage_layer.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"GraphSAGE Layer (Hamilton et al. 2017) — sage_layer","text":"aggregator S7 Aggregator object (e.g., MeanAggregator(), MaxAggregator()). aggregator responsible normalization. example, MeanAggregator() applies row normalization internally, MaxAggregator() uses adjacency structure without normalization.","code":""},{"path":[]},{"path":"https://josiahparry.github.io/torchgnn/reference/sage_layer.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"GraphSAGE Layer (Hamilton et al. 2017) — sage_layer","text":"Hamilton, W., Ying, Z., & Leskovec, J. (2017). Inductive representation learning large graphs. Advances Neural Information Processing Systems, 30. doi:10.48550/arXiv.1706.02216","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/sage_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Multi-layer GraphSAGE Model (Hamilton et al. 2017) — sage_model","title":"Multi-layer GraphSAGE Model (Hamilton et al. 2017) — sage_model","text":"Stacks multiple GraphSAGE layers configurable aggregation functions.","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/sage_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multi-layer GraphSAGE Model (Hamilton et al. 2017) — sage_model","text":"","code":"sage_model(   in_features,   hidden_dims,   out_features,   aggregator = MeanAggregator(),   activation = nnf_relu,   out_activation = NULL,   dropout = 0,   concat = TRUE )"},{"path":"https://josiahparry.github.io/torchgnn/reference/sage_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multi-layer GraphSAGE Model (Hamilton et al. 2017) — sage_model","text":"in_features Integer. Number input features per node hidden_dims Integer vector. Dimensions hidden layers (length = L) out_features Integer. Number output features (typically 1 regression) aggregator Aggregator S7 object. Aggregation function layers. Default: MeanAggregator() activation Function. Activation hidden layers. Default: nnf_relu out_activation Function NULL. Activation output layer. Default: NULL dropout Numeric. Dropout rate (0-1) applied hidden layer. Default: 0 concat Logical. TRUE, concatenates self neighbor features. FALSE, adds . Default: TRUE x Tensor n_nodes x in_features. Node feature matrix (dense sparse) adj Sparse torch tensor n_nodes x n_nodes. Adjacency matrix defining graph structure. Must sparse COO tensor.","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/sage_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Multi-layer GraphSAGE Model (Hamilton et al. 2017) — sage_model","text":"Tensor n_nodes x out_features. Final predictions","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/sage_model.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Multi-layer GraphSAGE Model (Hamilton et al. 2017) — sage_model","text":"Architecture: L hidden SAGE layers configurable activation 1 output SAGE layer optional output activation Total layers = length(hidden_dims) + 1 layer aggregates neighbor features using specified aggregator, combines self features via concatenation addition.","code":""},{"path":[]},{"path":"https://josiahparry.github.io/torchgnn/reference/sage_model.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Multi-layer GraphSAGE Model (Hamilton et al. 2017) — sage_model","text":"Hamilton, W., Ying, Z., & Leskovec, J. (2017). Inductive representation learning large graphs. Advances Neural Information Processing Systems, 30. doi:10.48550/arXiv.1706.02216","code":""},{"path":"https://josiahparry.github.io/torchgnn/reference/sage_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Multi-layer GraphSAGE Model (Hamilton et al. 2017) — sage_model","text":"","code":"if (FALSE) { # \\dontrun{ # Binary classification with sigmoid and mean aggregation model <- sage_model(14, c(56, 56), 1, output_activation = nnf_sigmoid)  # Multi-class with softmax and max aggregation model <- sage_model(   14,   c(32, 32),   10,   aggregator = MaxAggregator(),   output_activation = function(x) nnf_softmax(x, dim = -1) )  # Regression with sum aggregation model <- sage_model(14, c(64, 64), 1, aggregator = SumAggregator())  # With dropout and custom activation model <- sage_model(   14,   c(56, 56),   1,   activation = nnf_tanh,   dropout = 0.5 ) } # }"},{"path":"https://josiahparry.github.io/torchgnn/reference/torchgnn-package.html","id":null,"dir":"Reference","previous_headings":"","what":"torchgnn: Graph Neural Network Extensions for `torch` — torchgnn-package","title":"torchgnn: Graph Neural Network Extensions for `torch` — torchgnn-package","text":"package (one paragraph).","code":""},{"path":[]},{"path":"https://josiahparry.github.io/torchgnn/reference/torchgnn-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"torchgnn: Graph Neural Network Extensions for `torch` — torchgnn-package","text":"Maintainer: Josiah Parry josiah.parry@gmail.com (ORCID)","code":""}]
