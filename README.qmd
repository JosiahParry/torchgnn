---
title: torchgnn
format: gfm
---

Graph Neural Networks for [`{torch}`](https://mlverse.github.io/torch/) in R.

## Installation

Install the development version:

```{r, eval = FALSE}
pak::pak("josiahparry/torchgnn")
```

## Features

The following layers are implemented

- `gcn_conv_layer()`: Standard GCN layer (Kipf & Welling, 2016)
- `gcn_general_layer()`: Generalized GCN layer (Hamilton, 2020)
- `sage_layer()`: GraphSAGE layer (Hamilton, Ying, and Leskovec, 2017)
- `regconv_layer()`: RegionConv layer for regionalized GCN (Guo et al. 2025)



### Models

`{torchgnn}` provides utilities to create GNN models with multiple layers. 

- `gcn_conv_model()` 
- `gcn_general_model()` 
- `model_sage()`


```{r}
library(torchgnn)

gcn_general_model(
  in_features = 50,
  hidden_dims = c(32, 16),
  out_features = 1
)
```


### Aggregators 

An S7 class `torchgnn::Aggregator` is created with a generic method with signature `forward(x, adj, tensor)`. Only basic aggregators are implemented at present.

- `SumAggregator()`: Sum of neighbor features
- `MeanAggregator()`: Mean of neighbor features (with row normalization)
- `MaxAggregator()`: Element-wise maximum of neighbor features
- `MinAggregator()`: Element-wise minimum of neighbor features
- `ProductAggregator()`: Element-wise product of neighbor features
- `VarAggregator()`: Variance of neighbor features
- `StdAggregator()`: Standard deviation of neighbor features

```{r}
MeanAggregator()
```

### Data Preparation

`{torchgnn}` works with sparse COO torch tensors for adjacency and expects dense feature tensors. Note that weighted is handled by incorporating it directly into the adjacency matrix.

`adj_from_edgelist(from, to, weight)`: Create sparse adjacency matrix from edge list (supports character IDs, too)


```{r}
set.seed(0)
i <- sort(rep.int(1:10, 5))
j <- sample(1:10, 50, TRUE)

adj <- adj_from_edgelist(i, j)
adj
```

`nodes_to_tensor(nodes, adj, node_id)` will convert node features from a dataframe to a tensor with correct ordering.

```{r}
train_data <- as.data.frame(
  matrix(runif(10 * 5, max = 20), nrow = 10)
)

nodes_to_tensor(train_data, adj)
```

The function `graph_split()` create split **masks**. This API will likely change as how to integrate large graphs into `{torch}` and `{luz}` is figured out. 

```{r}
graph_split(adj)
```


## Example


Train a GCN on the PubMed Diabetes citation network for document classification.

This example creates a GCN model 

```{r message=FALSE}
#| code-fold: true
#| code-summary: data preparation
library(torch)
library(torchgnn)
library(nanoparquet)
library(dplyr)

# read to tempfiles
# data from https://linqs.org/datasets/#pubmed-diabetes
nodes_tmp <- tempfile("nodes", fileext = ".parquet")
edges_tmp <- tempfile("edges", fileext = ".parquet")

download.file(
  "https://github.com/JosiahParry/torchgnn/raw/refs/heads/main/examples/data/pubmed-diabetes/edges.parquet",
  edges_tmp, quiet = TRUE
)
download.file(
  "https://github.com/JosiahParry/torchgnn/raw/refs/heads/main/examples/data/pubmed-diabetes/nodes.parquet",
  nodes_tmp, quiet = TRUE
)

# read in the nodes and edges
nodes <- read_parquet(nodes_tmp)
edges <- read_parquet(edges_tmp)

# create our adjacency matrix from IDs
adj <- adj_from_edgelist(edges$from, edges$to)

# create our training data tensor
X <- nodes_to_tensor(
  select(nodes, -label),
  adj,
  node_id = "paper_id"
)

# create our target class tensor
Y <- nodes_to_tensor(select(nodes, 1:2), adj, "paper_id")$to(
  dtype = torch_long()
)$squeeze()

# perform a train, test, validation split
split <- graph_split(X, seed = 42)
```

```{r}
# define a 2 layer GCN model
model <- gcn_conv_model(
  # number of variables
  in_features = 500,
  # define our hidden layers
  hidden_dims = 16,
  # predicting 3 labels
  out_features = 3,
  dropout = 0.5
)
model 

# use ADAM optimizer
optimizer <- optim_adam(model$parameters, lr = 0.01)

# set our number of epochs
n_epochs <- 100

# create our training loop
for (epoch in 1:n_epochs) {
  model$train()
  optimizer$zero_grad()

  out <- model(X, adj)
  loss <- nnf_cross_entropy(out[split$train_id, ], Y[split$train_id])

  loss$backward()
  optimizer$step()

  model$eval()
  with_no_grad({
    val_out <- model(X, adj)
    val_pred <- val_out[split$val_id, ]$argmax(dim = 2)
    val_acc <- (val_pred == Y[split$val_id])$to(
      dtype = torch_float()
    )$mean()$item()
  })

  # print info every 20 epochs
  if (epoch %% 20 == 0) {
    cat(sprintf(
      "Epoch %d | Loss: %.4f | Val Acc: %.4f\n",
      epoch,
      loss$item(),
      val_acc
    ))
  }
}

model$eval()
with_no_grad({
  test_out <- model(X, adj)
  test_pred <- test_out[split$test_id, ]$argmax(dim = 2)
  test_acc <- (test_pred == Y[split$test_id])$to(
    dtype = torch_float()
  )$mean()$item()
})

cat(sprintf("Test Accuracy: %.4f\n", test_acc))
```

