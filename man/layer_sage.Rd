% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/layer-sage.R
\name{layer_sage}
\alias{layer_sage}
\title{GraphSAGE Layer (Hamilton et al. 2017)}
\usage{
layer_sage(
  in_features,
  out_features,
  aggregator = MeanAggregator(),
  bias = TRUE,
  concat = TRUE
)
}
\arguments{
\item{in_features}{Integer. Number of input features per node}

\item{out_features}{Integer. Number of output features per node}

\item{aggregator}{Aggregator S7 object. Default: \code{MeanAggregator()}}

\item{bias}{Logical. Add learnable bias. Default: TRUE}

\item{concat}{Logical. If TRUE, concatenates self and neighbor features. If FALSE, adds them. Default: TRUE}

\item{x}{Tensor \verb{n_nodes x in_features}. Node feature matrix (dense or sparse)}

\item{adj}{Sparse torch tensor \verb{n_nodes x n_nodes}. Adjacency matrix defining graph structure.
Must be a sparse COO tensor.}
}
\value{
Tensor \verb{n_nodes x out_features}. Transformed node features
}
\description{
Implements the GraphSAGE (Graph Sample and Aggregate) layer:

\deqn{\mathbf{h}_{\mathcal{N}(v)}^{(k)} = \text{AGGREGATE}\left(\{\mathbf{h}_u^{(k-1)} : u \in \mathcal{N}(v)\}\right)}

\deqn{\mathbf{h}_v^{(k)} = \sigma\left(\mathbf{W}^{(k)} \cdot \text{CONCAT}\left(\mathbf{h}_v^{(k-1)}, \mathbf{h}_{\mathcal{N}(v)}^{(k)}\right)\right)}

This layer:
\enumerate{
\item Aggregates neighbor features using the specified aggregator
\item Concatenates node's own features with aggregated neighbor features
\item Applies linear transformation and optional normalization
}

Parameters:
\itemize{
\item \eqn{W}: \verb{(in_features + aggregated_features) x out_features} learnable weight matrix
\item \eqn{b}: \code{out_features} learnable bias term (optional)
}
}
\details{
The aggregator should be an S7 Aggregator object (e.g., \code{MeanAggregator()}, \code{MaxAggregator()}).
Each aggregator is responsible for its own normalization. For example, \code{MeanAggregator()}
applies row normalization internally, while \code{MaxAggregator()} uses the adjacency structure
without normalization.
}
\section{Forward pass}{

}

\references{
Hamilton, W., Ying, Z., & Leskovec, J. (2017). Inductive representation learning
on large graphs. Advances in Neural Information Processing Systems, 30. \url{doi:10.48550/arXiv.1706.02216}
}
