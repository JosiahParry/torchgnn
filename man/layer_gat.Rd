% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/layer-gat.R
\name{layer_gat}
\alias{layer_gat}
\title{Graph Attention Network Layer (Veličković et al. 2018)}
\usage{
layer_gat(
  in_features,
  out_features,
  heads = 1,
  concat = TRUE,
  dropout = 0,
  negative_slope = 0.2,
  bias = TRUE
)
}
\arguments{
\item{in_features}{Integer. Number of input features per node}

\item{out_features}{Integer. Number of output features per node (per head)}

\item{heads}{Integer. Number of attention heads. Default: 1}

\item{concat}{Logical. If TRUE, concatenate multi-head outputs. If FALSE, average them.
Default: TRUE}

\item{dropout}{Numeric. Dropout rate (0-1) applied to attention coefficients. Default: 0}

\item{negative_slope}{Numeric. Negative slope for LeakyReLU. Default: 0.2}

\item{bias}{Logical. Add learnable bias. Default: TRUE}

\item{x}{Tensor \verb{n_nodes x in_features}. Node feature matrix}

\item{adj}{Sparse torch tensor \verb{n_nodes x n_nodes}. Adjacency matrix defining graph
structure. Must be a sparse COO tensor.}
}
\value{
Tensor \verb{n_nodes x (out_features * heads)} if concat=TRUE, else \verb{n_nodes x out_features}
}
\description{
Implements the Graph Attention Network (GAT) layer:

\deqn{\mathbf{h}_i^{(l+1)} = \sigma\left(\sum_{j \in \mathcal{N}(i)} \alpha_{ij} \mathbf{W}^{(l)} \mathbf{h}_j^{(l)}\right)}

where the attention coefficients \eqn{\alpha_{ij}} are computed as:

\deqn{\alpha_{ij} = \frac{\exp(\text{LeakyReLU}(\mathbf{a}^T [\mathbf{W}\mathbf{h}_i || \mathbf{W}\mathbf{h}_j]))}{\sum_{k \in \mathcal{N}(i)} \exp(\text{LeakyReLU}(\mathbf{a}^T [\mathbf{W}\mathbf{h}_i || \mathbf{W}\mathbf{h}_k]))}}

This layer:
\enumerate{
\item Applies linear transformation to node features
\item Computes attention coefficients for each edge
\item Normalizes attention weights via softmax over neighbors
\item Aggregates neighbor features weighted by attention
}

Parameters:
\itemize{
\item \eqn{W}: \verb{in_features x out_features} learnable weight matrix
\item \eqn{a}: \code{2 * out_features} learnable attention vector
}
}
\details{
Multi-head attention is supported via the \code{heads} parameter. When \code{heads > 1}:
\itemize{
\item If \code{concat = TRUE}: outputs are concatenated (output size = \code{out_features * heads})
\item If \code{concat = FALSE}: outputs are averaged (output size = \code{out_features})
}
}
\section{Forward pass}{

}

\references{
Veličković, P., Cucurull, G., Casanova, A., Romero, A., Liò, P., & Bengio, Y. (2018).
Graph Attention Networks. International Conference on Learning Representations.
\url{doi:10.48550/arXiv.1710.10903}
}
