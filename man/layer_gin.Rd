% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/layer-gin.R
\name{layer_gin}
\alias{layer_gin}
\title{Graph Isomorphism Network Layer (Xu et al. 2019)}
\usage{
layer_gin(in_features, out_features, eps = 0, learn_eps = FALSE)
}
\arguments{
\item{in_features}{Integer. Number of input features per node}

\item{out_features}{Integer. Number of output features per node}

\item{eps}{Numeric. Initial value for epsilon. Default: 0}

\item{learn_eps}{Logical. Whether to learn epsilon parameter. Default: FALSE}

\item{x}{Tensor \verb{n_nodes x in_features}. Node feature matrix}

\item{adj}{Sparse torch tensor \verb{n_nodes x n_nodes}. Adjacency matrix defining graph
structure. Must be a sparse COO tensor.}
}
\value{
Tensor \verb{n_nodes x out_features}. Transformed node features
}
\description{
Implements the Graph Isomorphism Network (GIN) layer:

\deqn{\mathbf{h}_i^{(k)} = \text{MLP}^{(k)}\left((1 + \epsilon^{(k)}) \cdot \mathbf{h}_i^{(k-1)} + \sum_{j \in \mathcal{N}(i)} \mathbf{h}_j^{(k-1)}\right)}

This layer:
\enumerate{
\item Aggregates neighbor features via summation
\item Adds weighted self features using learnable epsilon
\item Applies MLP transformation
}

Parameters:
\itemize{
\item MLP: Multi-layer perceptron (typically 2 layers)
\item epsilon: Learnable or fixed weight for self features
}
}
\details{
The MLP is constructed as a sequence of Linear-BatchNorm-ReLU-Linear layers.
The epsilon parameter can be learned or fixed at 0.
}
\section{Forward pass}{

}

\references{
Xu, K., Hu, W., Leskovec, J., & Jegelka, S. (2019). How Powerful are Graph
Neural Networks? International Conference on Learning Representations.
\url{doi:10.48550/arXiv.1810.00826}
}
