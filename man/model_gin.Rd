% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/model-gin.R
\name{model_gin}
\alias{model_gin}
\title{Multi-layer Graph Isomorphism Network Model (Xu et al. 2019)}
\usage{
model_gin(
  in_features,
  hidden_dims,
  out_features,
  eps = 0,
  learn_eps = FALSE,
  dropout = 0.5,
  out_activation = NULL
)
}
\arguments{
\item{in_features}{Integer. Number of input features per node}

\item{hidden_dims}{Integer vector. Dimensions of hidden layers (length = L)}

\item{out_features}{Integer. Number of output features (typically 1 for regression)}

\item{eps}{Numeric. Initial value for epsilon. Default: 0}

\item{learn_eps}{Logical. Whether to learn epsilon parameters. Default: FALSE}

\item{dropout}{Numeric. Dropout rate (0-1) applied between layers. Default: 0.5}

\item{out_activation}{Function or NULL. Activation for output layer. Default: NULL}

\item{x}{Tensor \verb{n_nodes x in_features}. Node feature matrix (dense or sparse)}

\item{adj}{Sparse torch tensor \verb{n_nodes x n_nodes}. Adjacency matrix defining graph
structure. Must be a sparse COO tensor.}
}
\value{
Tensor \verb{n_nodes x out_features}. Final predictions
}
\description{
Stacks multiple GIN layers with MLP transformations.
}
\details{
Architecture:
\itemize{
\item L hidden GIN layers with MLP transformations
\item 1 output GIN layer with optional output activation
\item Total layers = length(hidden_dims) + 1
}

Each layer aggregates neighbor features via summation, adds weighted self
features, and applies a 2-layer MLP transformation.
}
\section{Forward pass}{

}

\examples{
\dontrun{
# Binary classification
model <- model_gin(14, c(64, 64), 1, output_activation = nnf_sigmoid)

# Multi-class classification
model <- model_gin(
  14,
  c(64, 64),
  3,
  output_activation = function(x) nnf_softmax(x, dim = -1)
)

# With learnable epsilon
model <- model_gin(14, c(128), 1, learn_eps = TRUE)
}

}
\references{
Xu, K., Hu, W., Leskovec, J., & Jegelka, S. (2019). How Powerful are Graph
Neural Networks? International Conference on Learning Representations.
\url{doi:10.48550/arXiv.1810.00826}
}
